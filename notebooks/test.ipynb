{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Technical Guide: Implementing an AI Agent with LangGraph and Vector Search in Deno/TypeScript\n",
    "\n",
    "This guide outlines the implementation of a conversational AI agent with short-term and long-term memory capabilities using Deno/TypeScript, LangGraph, and MongoDB Atlas.\n",
    "\n",
    "## Tech Stack\n",
    "\n",
    "- Deno: JavaScript/TypeScript runtime\n",
    "- LangGraph: Framework for building stateful LLM applications\n",
    "- MongoDB Atlas: Cloud database with vector search capabilities\n",
    "- OpenAI Embeddings: For creating text vector representations\n",
    "- Anthropic's Claude: Language model for the conversational agent\n",
    "\n",
    "## Implementation Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Install dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "```bash\n",
    "deno add npm:zod \\\n",
    "npm:mongodb npm:@langchain/mongodb \\\n",
    "npm:langchain npm:@langchain/langgraph npm:@langchain/core npm:@langchain/community \\\n",
    "npm:@langchain/anthropic npm:@langchain/openai\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Environment Setup\n",
    "create an `.env` file with your tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "OPENAI_API_KEY = \"\";\n",
    "TAVILY_API_KEY = \"\";\n",
    "ANTHROPIC_API_KEY = \"\";\n",
    "MONGO_URI = \"\";\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load environment, then set up Mongo client and get your history collection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoints\n"
     ]
    }
   ],
   "source": [
    "import { load } from \"std/dotenv/mod.ts\";\n",
    "import { MongoClient } from \"mongodb\";\n",
    "\n",
    "const env: Record<string, string> = await load({ envPath: \"../.env\" });\n",
    "\n",
    "const { MONGO_URI } = env;\n",
    "\n",
    "const client = new MongoClient(MONGO_URI);\n",
    "const collection = client.db().collection(\"checkpoints\");\n",
    "\n",
    "console.log(collection.collectionName);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Vector Search Implementation\n",
    "\n",
    "First, create Atlas index see [langchain docs](https://js.langchain.com/v0.2/docs/integrations/vectorstores/mongodb_atlas/)\n",
    "\n",
    "\n",
    "```javascript\n",
    "    {\n",
    "        \"fields\": [\n",
    "            {\n",
    "            \"numDimensions\": 1024,\n",
    "            \"path\": \"embedding\",\n",
    "            \"similarity\": \"euclidean\",\n",
    "            \"type\": \"vector\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "```\n",
    "\n",
    "\n",
    "then prepare embeddings, we'll use OpenAI solution:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import { OpenAIEmbeddings } from \"@langchain/openai\";\n",
    "\n",
    "const { OPENAI_API_KEY } = env;\n",
    "\n",
    "// Initialize embeddings model\n",
    "export const embeddings1024 = new OpenAIEmbeddings({\n",
    "  model: \"text-embedding-3-small\",\n",
    "  dimensions: 1024,\n",
    "  apiKey: OPENAI_API_KEY,\n",
    "});\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { MongoDBAtlasVectorSearch } from \"npm:@langchain/mongodb@^0.0.4\";\n",
    "\n",
    "export const vectorStore = new MongoDBAtlasVectorSearch(embeddings1024, {\n",
    "  collection: collection,\n",
    "  indexName: \"default\",\n",
    "  textKey: \"messages.content\",\n",
    "  embeddingKey: \"embedding\",\n",
    "});\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, add function for history retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { z } from \"zod\";\n",
    "import { DynamicStructuredTool } from \"@langchain/core/tools\";\n",
    "\n",
    "export const getHistory = new DynamicStructuredTool({\n",
    "  name: \"get_history\",\n",
    "  description: \"Use text query to perform vector search against chat history\",\n",
    "  schema: z.object({\n",
    "    query: z.string(),\n",
    "  }),\n",
    "  func: async function ({ query }) {\n",
    "    console.log(\"\\x1b[33m%s\\x1b[0m\", `[DEBUG] history search: ${query}`);\n",
    "    const embededQuery = await embeddings1024.embedQuery(query);\n",
    "    const res = await vectorStore.similaritySearchVectorWithScore(\n",
    "      embededQuery,\n",
    "      3\n",
    "    );\n",
    "    const history = res\n",
    "      ?.map(\n",
    "        (rec: Array<Record<string, any>>) => rec?.[0]?.metadata.history || \"\"\n",
    "      )\n",
    "      .join(\"; \");\n",
    "    return history;\n",
    "  },\n",
    "});\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Adding memory to graph using MongoDB\n",
    "[Example of custom checkpointer implementation with Postgres](https://langchain-ai.github.io/langgraphjs/how-tos/persistence-postgres/?h=post)\n",
    "\n",
    "Here we extend our checkpoint with embeddings, before inserting them into mongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { MongoClient, Collection } from \"mongodb\";\n",
    "\n",
    "import { BaseMessage } from \"@langchain/core/messages\";\n",
    "import { RunnableConfig } from \"@langchain/core/runnables\";\n",
    "import { load } from \"@langchain/core/load\";\n",
    "\n",
    "import {\n",
    "  BaseCheckpointSaver,\n",
    "  Checkpoint,\n",
    "  CheckpointMetadata,\n",
    "  CheckpointTuple,\n",
    "} from \"npm:@langchain/langgraph@0.0.26\";\n",
    "\n",
    "const { MONGO_URI } = env;\n",
    "\n",
    "// define custom serializer for `checkpoint` and `metadata` values\n",
    "const CustomSerializer = {\n",
    "  stringify(obj: Record<string, unknown>) {\n",
    "    return JSON.stringify(obj);\n",
    "  },\n",
    "\n",
    "  async parse(data: string) {\n",
    "    return await load(data.toString());\n",
    "  },\n",
    "};\n",
    "\n",
    "interface CheckpointRecord {\n",
    "  checkpoint: string;\n",
    "  metadata: string;\n",
    "  parent_id?: string;\n",
    "  thread_id: string;\n",
    "  checkpoint_id: string;\n",
    "  embedding: number[];\n",
    "  history: string;\n",
    "  timestamp: Date;\n",
    "}\n",
    "\n",
    "export class MongoSaver extends BaseCheckpointSaver {\n",
    "  private client: MongoClient;\n",
    "  private isSetup: boolean;\n",
    "  public collection: Collection<CheckpointRecord>;\n",
    "\n",
    "  constructor(client: MongoClient) {\n",
    "    super(CustomSerializer);\n",
    "    this.client = client;\n",
    "    this.collection = this.client.db().collection(\"checkpoints\");\n",
    "    this.isSetup = false;\n",
    "  }\n",
    "\n",
    "  static fromConnString(connString: string = MONGO_URI || \"\"): MongoSaver {\n",
    "    return new MongoSaver(new MongoClient(connString));\n",
    "  }\n",
    "\n",
    "  private async setup(): Promise<void> {\n",
    "    if (this.isSetup) return;\n",
    "\n",
    "    try {\n",
    "      await this.collection.findOne();\n",
    "      this.isSetup = true;\n",
    "    } catch (error) {\n",
    "      console.error(\"Error querying checkpoints collection\", error);\n",
    "      throw error;\n",
    "    }\n",
    "  }\n",
    "\n",
    "  // below 3 methods are necessary for any checkpointer implementation: getTuple, list and put\n",
    "  async getTuple(config: RunnableConfig): Promise<CheckpointTuple | undefined> {\n",
    "    await this.setup();\n",
    "    const { thread_id, checkpoint_id } = config.configurable || {};\n",
    "\n",
    "    try {\n",
    "      if (checkpoint_id) {\n",
    "        const res = await this.collection.findOne({ thread_id, checkpoint_id });\n",
    "\n",
    "        if (res) {\n",
    "          const mappedRes = {\n",
    "            config,\n",
    "            checkpoint: (await this.serde.parse(res.checkpoint)) as Checkpoint,\n",
    "            metadata: (await this.serde.parse(\n",
    "              res.metadata\n",
    "            )) as CheckpointMetadata,\n",
    "            parentConfig: res.parent_id\n",
    "              ? {\n",
    "                  configurable: {\n",
    "                    thread_id,\n",
    "                    checkpoint_id: res.parent_id,\n",
    "                  },\n",
    "                }\n",
    "              : undefined,\n",
    "          };\n",
    "\n",
    "          return mappedRes;\n",
    "        }\n",
    "      } else {\n",
    "        const res = await this.collection.findOne(\n",
    "          { thread_id },\n",
    "          { sort: { timestamp: -1 } }\n",
    "        );\n",
    "\n",
    "        if (res) {\n",
    "          const mappedRes = {\n",
    "            config: {\n",
    "              configurable: {\n",
    "                thread_id: res.thread_id,\n",
    "                checkpoint_id: res.checkpoint_id,\n",
    "              },\n",
    "            },\n",
    "            checkpoint: (await this.serde.parse(res.checkpoint)) as Checkpoint,\n",
    "            metadata: (await this.serde.parse(\n",
    "              res.metadata\n",
    "            )) as CheckpointMetadata,\n",
    "            parentConfig: res.parent_id\n",
    "              ? {\n",
    "                  configurable: {\n",
    "                    thread_id: res.thread_id,\n",
    "                    checkpoint_id: res.parent_id,\n",
    "                  },\n",
    "                }\n",
    "              : undefined,\n",
    "          };\n",
    "\n",
    "          return mappedRes;\n",
    "        }\n",
    "      }\n",
    "    } catch (error) {\n",
    "      console.error(\"Error retrieving checkpoint\", error);\n",
    "      throw error;\n",
    "    }\n",
    "\n",
    "    return undefined;\n",
    "  }\n",
    "\n",
    "  async *list(\n",
    "    config: RunnableConfig,\n",
    "    limit?: number,\n",
    "    before?: RunnableConfig\n",
    "  ): AsyncGenerator<CheckpointTuple> {\n",
    "    await this.setup();\n",
    "    const { thread_id } = config.configurable || {};\n",
    "    let query: Record<string, unknown> = { thread_id };\n",
    "\n",
    "    const params: (string | number)[] = [thread_id];\n",
    "    if (before?.configurable?.checkpoint_id) {\n",
    "      query = {\n",
    "        ...query,\n",
    "        checkpoint_id: { $lt: before.configurable.checkpoint_id },\n",
    "      };\n",
    "      params.push(before.configurable.checkpoint_id);\n",
    "    }\n",
    "    let options: Record<string, unknown> = { sort: { timestamp: -1 } };\n",
    "    if (limit) {\n",
    "      query.limit = params.length + 1;\n",
    "      params.push(limit);\n",
    "    }\n",
    "\n",
    "    try {\n",
    "      const res = await this.collection.find(query, options).toArray();\n",
    "\n",
    "      for (const row of res) {\n",
    "        yield {\n",
    "          config: {\n",
    "            configurable: {\n",
    "              thread_id: row.thread_id,\n",
    "              checkpoint_id: row.checkpoint_id,\n",
    "            },\n",
    "          },\n",
    "          checkpoint: (await this.serde.parse(row.checkpoint)) as Checkpoint,\n",
    "          metadata: (await this.serde.parse(\n",
    "            row.metadata\n",
    "          )) as CheckpointMetadata,\n",
    "          parentConfig: row.parent_id\n",
    "            ? {\n",
    "                configurable: {\n",
    "                  thread_id: row.thread_id,\n",
    "                  checkpoint_id: row.parent_id,\n",
    "                },\n",
    "              }\n",
    "            : undefined,\n",
    "        };\n",
    "      }\n",
    "    } catch (error) {\n",
    "      console.error(\"Error listing checkpoints\", error);\n",
    "      throw error;\n",
    "    }\n",
    "  }\n",
    "\n",
    "  async put(\n",
    "    config: RunnableConfig,\n",
    "    checkpoint: Checkpoint,\n",
    "    metadata: CheckpointMetadata\n",
    "  ): Promise<RunnableConfig> {\n",
    "    await this.setup();\n",
    "    try {\n",
    "      const messages =\n",
    "        (checkpoint?.channel_values?.messages as BaseMessage[]) || [];\n",
    "      const lastMessage = messages?.[messages?.length - 1] || {};\n",
    "\n",
    "      let text: string;\n",
    "\n",
    "      if (lastMessage.content instanceof Array) {\n",
    "        const message: Record<string, unknown> =\n",
    "          lastMessage.content.find((message) => message.type === \"text\") || {};\n",
    "        text = message?.text as string;\n",
    "      } else {\n",
    "        text = lastMessage.content;\n",
    "      }\n",
    "\n",
    "      if (text) {\n",
    "        const embeddings = await embeddings1024.embedDocuments([text]);\n",
    "\n",
    "        const update: CheckpointRecord = {\n",
    "          thread_id: config?.configurable?.thread_id,\n",
    "          checkpoint_id: checkpoint.id,\n",
    "          parent_id: config?.configurable?.checkpoint_id,\n",
    "          checkpoint: this.serde.stringify(checkpoint),\n",
    "          metadata: this.serde.stringify(metadata),\n",
    "          embedding: embeddings[0] || [],\n",
    "          history: text,\n",
    "          timestamp: new Date(),\n",
    "        };\n",
    "\n",
    "        await this.collection.insertOne(update);\n",
    "      }\n",
    "    } catch (error) {\n",
    "      console.error(\"Error saving checkpoint\", error);\n",
    "      throw error;\n",
    "    }\n",
    "\n",
    "    return {\n",
    "      configurable: {\n",
    "        thread_id: config?.configurable?.thread_id,\n",
    "        checkpoint_id: checkpoint?.id,\n",
    "      },\n",
    "    };\n",
    "  }\n",
    "\n",
    "  async closeConnection(): Promise<void> {\n",
    "    await this.client.close();\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Conversation Management with LangGraph\n",
    "\n",
    "First, we need to create tools, we'll use TavilySearch for the internet acsess and our `getHistory` function as long term memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { TavilySearchResults } from \"@langchain/community/tools/tavily_search\";\n",
    "\n",
    "const { TAVILY_API_KEY } = env;\n",
    "\n",
    "const searchTool = new TavilySearchResults({\n",
    "  maxResults: 1,\n",
    "  apiKey: TAVILY_API_KEY,\n",
    "});\n",
    "\n",
    "const tools = [getHistory, searchTool];\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then define the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import {\n",
    "  HumanMessage,\n",
    "  AIMessage,\n",
    "  SystemMessage,\n",
    "} from \"@langchain/core/messages\";\n",
    "import { ChatAnthropic } from \"@langchain/anthropic\";\n",
    "import { END, START, StateGraph, StateGraphArgs } from \"@langchain/langgraph\";\n",
    "import { ToolNode } from \"@langchain/langgraph/prebuilt\";\n",
    "\n",
    "// Define the state interface\n",
    "interface AgentState {\n",
    "  messages: HumanMessage[];\n",
    "}\n",
    "\n",
    "// Define the graph state\n",
    "const graphState: StateGraphArgs<AgentState>[\"channels\"] = {\n",
    "  messages: {\n",
    "    value: (x: HumanMessage[], y: HumanMessage[]) => x.concat(y),\n",
    "    default: () => [\n",
    "      new SystemMessage(\n",
    "        `You are helpful assistent. You have memory shared between conversations, stored in database\\n\n",
    "          Please, check current messages state first!\\n\n",
    "          If you still miss it please don't respond, until you tried to use get_history tool. Prompt it for vector searh.\\n\n",
    "          This is start of the new conversation`\n",
    "      ),\n",
    "    ],\n",
    "  },\n",
    "};\n",
    "\n",
    "const { ANTHROPIC_API_KEY } = env;\n",
    "\n",
    "// initialize model\n",
    "const model = new ChatAnthropic({\n",
    "  model: \"claude-3-sonnet-20240229\",\n",
    "  temperature: 0,\n",
    "  apiKey: ANTHROPIC_API_KEY,\n",
    "});\n",
    "\n",
    "// Bind tools to the model\n",
    "const toolNode = new ToolNode(tools);\n",
    "const boundModel = model.bindTools(tools);\n",
    "\n",
    "// Define the function that determines whether to continue or not\n",
    "function shouldContinue(state: AgentState): \"tools\" | typeof END {\n",
    "  const messages = state.messages;\n",
    "  const lastMessage = messages[messages.length - 1] as AIMessage;\n",
    "\n",
    "  // If the LLM makes a tool call, then we route to the \"tools\" node\n",
    "  if (lastMessage.tool_calls?.length) {\n",
    "    return \"tools\";\n",
    "  }\n",
    "  // Otherwise, we stop (reply to the user)\n",
    "  return END;\n",
    "}\n",
    "\n",
    "// Define the function that calls the model\n",
    "async function callModel(state: AgentState, congig: RunnableConfig) {\n",
    "  const messages = state.messages;\n",
    "\n",
    "  const response = await boundModel.invoke(messages, config);\n",
    "\n",
    "  // We return a list, because this will get added to the existing list\n",
    "  return { messages: [response] };\n",
    "}\n",
    "\n",
    "// Define a new graph\n",
    "const workflow = new StateGraph<AgentState>({ channels: graphState })\n",
    "  .addNode(\"agent\", callModel)\n",
    "  .addNode(\"tools\", toolNode)\n",
    "\n",
    "  .addEdge(START, \"agent\")\n",
    "  .addConditionalEdges(\"agent\", shouldContinue)\n",
    "  .addEdge(\"tools\", \"agent\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Define user interface\n",
    "We'll use function that accepts compiled graph and user message,\n",
    "at the very first call it will put system message to instruct model how to use tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { HumanMessage } from \"@langchain/core/messages\";\n",
    "import { CompiledStateGraph } from \"@langchain/langgraph\";\n",
    "\n",
    "async function writeUserMessage(\n",
    "  app: CompiledStateGraph<AgentState>,\n",
    "  userMessage: string\n",
    ") {\n",
    "  try {\n",
    "    console.log(\"\\x1b[31m%s\\x1b[0m\", userMessage);\n",
    "\n",
    "    const inputs = {\n",
    "      messages: [new HumanMessage(userMessage)],\n",
    "    };\n",
    "\n",
    "    for await (const event of await app.stream(inputs, {\n",
    "      streamMode: \"values\",\n",
    "    })) {\n",
    "      const lastMessage = event.messages[event.messages.length - 1];\n",
    "      // console.log(\"\\x1b[32m%s\\x1b[0m\", 'DEBUG', lastMessage)\n",
    "      if (lastMessage.tool_calls?.length === 0) {\n",
    "        // final answer\n",
    "        console.log(\"\\x1b[36m%s\\x1b[0m\", lastMessage.content);\n",
    "      }\n",
    "    }\n",
    "  } catch (e) {\n",
    "    console.log(e);\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Testing the AI Agent\n",
    "\n",
    "Start a new conversation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{ acknowledged: \u001b[33mtrue\u001b[39m, deletedCount: \u001b[33m40\u001b[39m }"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import { ObjectId } from \"mongodb\";\n",
    "\n",
    "// cleanup history\n",
    "await collection.deleteMany({});\n",
    "\n",
    "const checkpointer = MongoSaver.fromConnString();\n",
    "\n",
    "let config = {\n",
    "  configurable: {\n",
    "    thread_id: new ObjectId().valueOf().toString(),\n",
    "  },\n",
    "};\n",
    "\n",
    "const app: CompiledStateGraph<AgentState> = workflow\n",
    "  .compile({ checkpointer })\n",
    "  .withConfig(config);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test short-term memory, getting previous message from the graph state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mHello. My name is vasily\u001b[0m\n",
      "\u001b[33m[DEBUG] history search: vasily\u001b[0m\n",
      "\u001b[36mHello Vasily, it's nice to meet you! Since this is the start of a new conversation, I don't have any prior context about you. Please feel free to ask me anything or let me know if there is a particular topic you'd like to discuss. I'm an AI assistant created by Anthropic to be helpful, honest and harmless.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "await writeUserMessage(app, \"Hello. My name is vasily\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mdo you remember my name\u001b[0m\n",
      "\u001b[36mYes, I remember your name is Vasily.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "await writeUserMessage(app, \"do you remember my name\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mwhat is current weather in ny\u001b[0m\n",
      "\u001b[36mAccording to the latest search results, the current weather in New York City today is sunny with a temperature of around 88°F (31°C). The wind is light from the west-southwest at around 4-6 mph. Overall it appears to be a warm and sunny summer day in NYC.\n",
      "\n",
      "Let me know if you need any other details about the current weather conditions there!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "await writeUserMessage(app, \"what is current weather in ny\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mTell me a story\u001b[0m\n",
      "\u001b[36mHere's a little story for you Vasily:\n",
      "\n",
      "Once upon a time, there was a young adventurer named Alex who loved exploring the great outdoors. One summer, Alex decided to go on a hiking trip through the mountains. \n",
      "\n",
      "After driving for hours down winding roads, Alex finally arrived at the trailhead. With a backpack full of supplies, Alex set off into the wilderness. The trail wound its way up the mountainside, through fields of wildflowers and past babbling brooks.\n",
      "\n",
      "As Alex climbed higher, the trees thinned out and the views became more and more breathtaking. Snow-capped peaks rose majestically in the distance. Alex stopped to catch their breath and admire the scenery.\n",
      "\n",
      "Suddenly, a rustling came from the bushes nearby. Alex froze, wondering if it was a bear or other wild animal. But then a cute little chipmunk scampered out onto the trail! It stood up on its hind legs, stuffing its cheeks with seeds and nuts before scurrying away.\n",
      "\n",
      "Alex laughed at the silly chipmunk and continued on the trail, feeling grateful to be out in nature away from the hustle and bustle of the city. Who knows what other adventures the day would bring?\n",
      "\n",
      "I hope you enjoyed that little story, Vasily! I aimed to create a lighthearted narrative about an adventurer exploring the mountains. Let me know if you'd like to hear another story sometime.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "await writeUserMessage(app, \"Tell me a story\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's start new conversation thread and see if it remembers something!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "let config = {\n",
    "  configurable: {\n",
    "    thread_id: new ObjectId().valueOf().toString(),\n",
    "  },\n",
    "};\n",
    "\n",
    "const app: CompiledStateGraph<AgentState> = workflow\n",
    "  .compile({ checkpointer })\n",
    "  .withConfig(config);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mWhat was the story\u001b[0m\n",
      "\u001b[33m[DEBUG] history search: story\u001b[0m\n",
      "\u001b[36mBased on the search results, it seems the story I previously told was about a young adventurer named Alex who went on a hiking trip through the mountains. The story described Alex's journey along the trail, encountering things like wildflowers, streams, mountain views, and a cute chipmunk. It was meant to be a lighthearted, descriptive tale capturing the wonder of being out in nature.\n",
      "\n",
      "Since you asked \"What was the story\", I've provided a summary of the key details from the story I had told earlier in our conversation. Please let me know if you need any clarification or have additional questions!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "await writeUserMessage(app, \"What was the story\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mI asked you about weather before, please remind me\u001b[0m\n",
      "\u001b[33m[DEBUG] history search: weather\u001b[0m\n",
      "\u001b[36mUnfortunately, I could not find any previous messages in our conversation history about you asking me about the weather before this current exchange. The search results only show that you asked \"what is current weather in ny\", to which I provided the current weather details for New York City.\n",
      "\n",
      "If you had asked about weather at an earlier point, those messages do not seem to be present in our conversation history that I have access to. Please let me know if you have any other details that could help me locate a previous weather-related query from you. Otherwise, I do not have enough context to remind you about a prior weather discussion.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "await writeUserMessage(\n",
    "  app,\n",
    "  \"I asked you about weather before, please remind me\"\n",
    ");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as we can see the Agent is capable of seaching both \"short\" and \"long\" memory with help of the MongoDB!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deno",
   "language": "typescript",
   "name": "deno"
  },
  "language_info": {
   "codemirror_mode": "typescript",
   "file_extension": ".ts",
   "mimetype": "text/x.typescript",
   "name": "typescript",
   "nbconvert_exporter": "script",
   "pygments_lexer": "typescript",
   "version": "5.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
